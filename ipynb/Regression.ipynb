{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up spark context and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "## set up spark context\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "## set up  SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').load(\"./data/Advertising.csv\",header=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the dataset and the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.take(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert the data to dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the data to dense vector\n",
    "def transData(row):\n",
    "    return Row(label=row[\"Sales\"],\n",
    "               features=Vectors.dense([\n",
    "                                       row[\"TV\"],\n",
    "                                       row[\"Radio\"],\n",
    "                                       row[\"Newspaper\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transform the dataset to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "|  [8.7,48.9,75.0]|  7.2|\n",
      "| [57.5,32.8,23.5]| 11.8|\n",
      "|[120.2,19.6,11.6]| 13.2|\n",
      "|    [8.6,2.1,1.0]|  4.8|\n",
      "| [199.8,2.6,21.2]| 10.6|\n",
      "|  [66.1,5.8,24.2]|  8.6|\n",
      "| [214.7,24.0,4.0]| 17.4|\n",
      "| [23.8,35.1,65.9]|  9.2|\n",
      "|   [97.5,7.6,7.2]|  9.7|\n",
      "|[204.1,32.9,46.0]| 19.0|\n",
      "|[195.4,47.7,52.9]| 22.4|\n",
      "|[67.8,36.6,114.0]| 12.5|\n",
      "|[281.4,39.6,55.8]| 24.4|\n",
      "| [69.2,20.5,18.3]| 11.3|\n",
      "|[147.3,23.9,19.1]| 14.6|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = df.rdd.map(transData).toDF() \n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. Fit model (Ridge Regression and the LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit 2 models, using different regularization parameters\n",
    "modelA = lr.fit(transformed, {lr.regParam:0.0})\n",
    "#modelB = lr.fit(transformed, {lr.regParam:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0458, 0.1885, -0.001])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelA.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9388893694594134"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " modelA.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # Make predictions\n",
    "predictionsA = modelA.transform(transformed)\n",
    "predictionsA.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "RMSE = evaluator.evaluate(predictionsA)\n",
    "print(\"ModelA: Root Mean Squared Error = \" + str(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictionsB = modelB.transform(transformed)\n",
    "predictionsB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RMSE = evaluator.evaluate(predictionsB)\n",
    "print(\"ModelB: Root Mean Squared Error = \" + str(RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import numpy, pandas, and ggplot\n",
    "import numpy as np\n",
    "from pandas import *\n",
    "from ggplot import *\n",
    " \n",
    "# Create Python DataFrame\n",
    "pop = transformed.rdd.map(lambda p: (p.features[0])).collect()\n",
    "sales = transformed.rdd.map(lambda p: (p.label)).collect()\n",
    "predA = predictionsA.select(\"prediction\").rdd.map(lambda r: r[0]).collect()\n",
    "predB = predictionsB.select(\"prediction\").rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "\n",
    "\n",
    "pydf = DataFrame([predA]) \n",
    "nx,ny = pydf.shape\n",
    "type1 = Series([0 for x in range(ny)])\n",
    "type2 = Series([1 for x in range(ny)])\n",
    "\n",
    "#pydf\n",
    "# pandas DataFrame    \n",
    "pydf1 = DataFrame({'pop':pop,'sales':sales,'pred':predA,'type':type1})\n",
    "pydf2 = DataFrame({'pop':pop,'sales':sales,'pred':predB,'type':type2})\n",
    "\n",
    "frames = [pydf1, pydf2]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "result['type'] = result['type'].astype(object)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create scatter plot and two regression models (scaling exponential) using ggplot\n",
    "ggplot(result, aes(x='pop',y='pred',color='type')) +\\\n",
    "geom_point(colors='blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. More features about the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').load(\"./data/Advertising.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the data to dense vector\n",
    "def transData(row):\n",
    "    return Row(label=row[\"Sales\"],\n",
    "               features=Vectors.dense([\n",
    "                                       row[\"TV\"],\n",
    "                                       row[\"Radio\"],\n",
    "                                       row[\"Newspaper\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=5, regParam=0.0, solver=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed = df.rdd.map(transData).toDF() \n",
    "#transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lr.fit(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0458, 0.1885, -0.001])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9388893694594134"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save and extract model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_path = 'temp/Users/wenqiangfeng/Dropbox/Spark/Code/model'\n",
    "modelPath = temp_path + \"/lr_model\"\n",
    "model.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr2 = model.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0458, 0.1885, -0.001])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.coefficients"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
